{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the project work\n",
    "\n",
    "Steps:\n",
    "\n",
    "0. Import packages\n",
    "1. Loading the data \n",
    "2. Ploting an example picture \n",
    "3. Creating the network\n",
    "4. training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the data\n",
    "\n",
    "-Looks like the data is already splited when we get it so we don't need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = './skogsstyrelsen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For use with files in same directry locally possibly\n",
    "# Read data + corresponding json info (incl ground truth)\n",
    "img_paths_train = list(np.load('skogs_names_train.npy'))\n",
    "img_paths_val = list(np.load( 'skogs_names_val.npy'))\n",
    "#img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "json_content_train = np.load('skogs_json_train.npy', allow_pickle=True)\n",
    "json_content_val = list(np.load('skogs_json_val.npy', allow_pickle=True))\n",
    "#json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))\n",
    "#print(img_paths_train)\n",
    "\n",
    "#print(img_paths_val)\n",
    "#print(json_content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './skogsstyrelsen/skogs_names_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read data + corresponding json info (incl ground truth) (for on github usage)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m img_paths_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      3\u001b[0m img_paths_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m      4\u001b[0m img_paths_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_PATH_DATA, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskogs_names_test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './skogsstyrelsen/skogs_names_train.npy'"
     ]
    }
   ],
   "source": [
    "# Read data + corresponding json info (incl ground truth) (for on github usage)\n",
    "img_paths_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy')))\n",
    "img_paths_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy')))\n",
    "img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "json_content_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_train.npy'), allow_pickle=True))\n",
    "json_content_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_val.npy'), allow_pickle=True))\n",
    "json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ncplot' from 'ncplot' (C:\\Users\\adam2\\AppData\\Roaming\\Python\\Python311\\site-packages\\ncplot\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mncplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ncplot\n\u001b[0;32m      2\u001b[0m ncplot(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskgs_1f16b04c-36c7-ed11-9174-005056a6f472.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ncplot' from 'ncplot' (C:\\Users\\adam2\\AppData\\Roaming\\Python\\Python311\\site-packages\\ncplot\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from ncplot import ncplot\n",
    "ncplot(\"skgs_1f16b04c-36c7-ed11-9174-005056a6f472.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Get the training split of the MNIST dataset, the ToTensor makes sure we get the data as tensors, not images\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m mnist_train \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtorchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[0;32m      4\u001b[0m mnist_test \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtorchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create a DataLoader from the dataset that we'll use to get batches of data during training\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the training split of the MNIST dataset, the ToTensor makes sure we get the data as tensors, not images\n",
    "mnist_train = torchvision.datasets.MNIST('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('./', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "# Create a DataLoader from the dataset that we'll use to get batches of data during training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=1000, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ploting an example picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes an MNIST tensor and shows the image\n",
    "def plot_digit(data):\n",
    "    # Transfrom the images into an appropriate shape for displaying\n",
    "    data = data.view(28,28)\n",
    "    plt.imshow(data, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Get the first batch of images and labels from the DataLoader\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Plot the first image of the batch\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating the network\n",
    "\n",
    "-\"CrossEntropyLoss expects class indices and does not take one-hot encoded tensors as target labels.\" pytorch forums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------Network--------------------------------------\n",
    "\n",
    "epochs=300\n",
    "network=nn.Sequential(\n",
    "    nn.Linear(784, 80), # First layer of the network takes the entire image and reduces it to 100 dimensions\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(80, 10) # The second layer takes those 100 dimensions and reduces them into estimeated values for each digit\n",
    ")\n",
    "\n",
    "loss_function=torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the number of epochs to train for (one epoch is one optimization iteration on the entire dataset)\n",
    "epochs = 10\n",
    "losses=[]\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # For each batch of data (since the dataset is too large to run all data through the network at once)\n",
    "    for (images, labels) in train_loader:\n",
    "\n",
    "        # Extract the labels and turn them into one-hot representation (note: not all loss functions needs this)\n",
    "\n",
    "        # Reshape the images to a single vector (28*28 = 784)\n",
    "        images = images.view(-1,784)\n",
    "\n",
    "        # Predict for each vector what digit they represent\n",
    "        prediction = network(data)\n",
    "\n",
    "        # Calculate the loss of the prediction by comparing to the expected output\n",
    "        loss = loss_function(prediction, labels)\n",
    "\n",
    "        # Backpropogate the loss through the network to find the gradients of all parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters along their gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clear stored gradient values\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "        losses.append(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

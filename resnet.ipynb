{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import json\n",
    "import copy\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import v2\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = 'data/skogsstyrelsen/'\n",
    "BAND_NAMES = ['b01', 'b02', 'b03', 'b04', 'b05', 'b06', 'b07', 'b08', 'b8a', 'b09', 'b11', 'b12']\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data + corresponding json info (incl ground truth)\n",
    "img_paths_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy')))\n",
    "img_paths_train = [path[1:] for path in img_paths_train]\n",
    "\n",
    "img_paths_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy')))\n",
    "img_paths_val = [path[1:] for path in img_paths_val]\n",
    "\n",
    "img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "img_paths_test = [path[1:] for path in img_paths_test]\n",
    "\n",
    "json_content_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_train.npy'), allow_pickle=True))\n",
    "json_content_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_val.npy'), allow_pickle=True))\n",
    "json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))\n",
    "\n",
    "train_label = list(np.load(os.path.join(BASE_PATH_DATA, \"skogs_gts_train.npy\")))\n",
    "val_label = list(np.load(os.path.join(BASE_PATH_DATA, \"skogs_gts_val.npy\")))\n",
    "test_label = list(np.load(os.path.join(BASE_PATH_DATA, \"skogs_gts_test.npy\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = xr.open_dataset(path)\n",
    "    yy_mm_dd = getattr(img, 'time').values[0]\n",
    "    yy = yy_mm_dd.astype('datetime64[Y]').astype(int) + 1970\n",
    "    mm = yy_mm_dd.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "\n",
    "    band_list = []\n",
    "    for band in BAND_NAMES:\n",
    "        if yy >= 2022 and mm >= 1: # New normalization after Jan 2022\n",
    "            band_list.append((getattr(img, band).values - 1000) / 10000)\n",
    "        else:\n",
    "            band_list.append(getattr(img, band).values / 10000) \n",
    "            \n",
    "    img = np.concatenate(band_list, axis = 0)\n",
    "    img = np.transpose(img, [1,2,0])\n",
    "    img = np.fliplr(img).copy()\n",
    "    img = np.flipud(img).copy()\n",
    "\n",
    "    H, W = img.shape[:2]\n",
    "        \n",
    "    # padding\n",
    "    if H != 21 and W != 21:\n",
    "        zeros = np.zeros((1, 20, 12))\n",
    "        img = np.concatenate((img, zeros), axis = 0)\n",
    "        zeros = np.zeros((21, 1, 12))\n",
    "        img = np.concatenate((img, zeros[:]), axis = 1)\n",
    "        \n",
    "    elif H != 21:\n",
    "        zeros = np.zeros((1, 21, 12))\n",
    "        img = np.concatenate((img, zeros), axis = 0)\n",
    "        \n",
    "    elif W != 21:\n",
    "        zeros = np.zeros((21, 1, 12))\n",
    "        img = np.concatenate((img, zeros[:]), axis = 1)\n",
    "        \n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase we want to use a Dataloader, we could use this\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, label_dir, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = list(np.load(label_dir))\n",
    "        self.img_dir = img_dir\n",
    "        image_paths = list(np.load(img_dir))\n",
    "        self.image_paths = [path[1:] for path in image_paths]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = load_image(self.image_paths[idx])\n",
    "        \n",
    "        # convert to float32\n",
    "        image = np.float32(image)\n",
    "        label = self.img_labels[idx]\n",
    "        #image = image[:, :, [0,1,5,9,10,11]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "SHUFFLE = False\n",
    "\n",
    "# Train augmentations\n",
    "transformation = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    #v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=utils.MEANS, std=utils.STDS),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Validation augmentation\n",
    "transformation_val = v2.Compose([\n",
    "    v2.Normalize(mean=utils.MEANS, std=utils.STDS),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_data = CustomImageDataset(os.path.join(BASE_PATH_DATA, \"skogs_gts_train.npy\"), os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy'), transform=transformation)\n",
    "labels = train_data.img_labels\n",
    "class_sample_count = np.array(\n",
    "    [len(np.where(labels == t)[0]) for t in np.unique(labels)])\n",
    "\n",
    "weight = 1. / class_sample_count\n",
    "samples_weight = np.array([weight[t] for t in labels])\n",
    "\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE, sampler=sampler)\n",
    "\n",
    "val_data = CustomImageDataset(os.path.join(BASE_PATH_DATA, \"skogs_gts_val.npy\"), os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy'), transform=transformation_val)\n",
    "val_loader = DataLoader(val_data, batch_size=len(val_data), shuffle=SHUFFLE)\n",
    "\n",
    "test_data = CustomImageDataset(os.path.join(BASE_PATH_DATA, \"skogs_gts_test.npy\"), os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy'), transform=transformation_val)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs = 10, show_plot = True):\n",
    "    \n",
    "    min_loss = 10000\n",
    "    max_acc = 0\n",
    "    # Track loss\n",
    "    training_loss, validation_loss = [], []\n",
    "    \n",
    "    # Track accuracy\n",
    "    training_acc, validation_acc = [], []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        # Track loss\n",
    "        epoch_training_loss, epoch_validation_loss = 0, 0\n",
    "        train_size, val_size = 0, 0\n",
    "        \n",
    "        # track accuracy\n",
    "        train_correct, val_correct = 0, 0\n",
    "\n",
    "        # training\n",
    "        model.train(True)\n",
    "        for batch_nr, (data, labels) in enumerate(train_loader):\n",
    "            # predict\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(data)\n",
    "            pred = torch.squeeze(pred)\n",
    "\n",
    "            # calculate accuracy\n",
    "            preds = torch.sigmoid(pred) >= 0.5\n",
    "            \n",
    "            train_correct += torch.sum(preds==labels).item()\n",
    "            \n",
    "            # Clear stored gradient values\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(pred, labels.float())\n",
    "            \n",
    "            # Backpropagate the loss through the network to find the gradients of all parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters along their gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update loss\n",
    "            epoch_training_loss += loss.cpu().detach().numpy()\n",
    "            train_size = batch_nr\n",
    "            \n",
    "        # validation\n",
    "        model.eval()\n",
    "        for data, labels in val_loader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # predict\n",
    "\n",
    "            pred = model(data)\n",
    "            pred = torch.squeeze(pred)\n",
    "\n",
    "            # calculate accuracy\n",
    "            preds = torch.sigmoid(pred) >= 0.5\n",
    "            \n",
    "            val_correct += torch.sum(preds==labels).item()\n",
    "             \n",
    "            # calculate loss\n",
    "            loss = criterion(pred, labels.float())\n",
    "            \n",
    "            # check if loss is smaller than before, if so safe model\n",
    "            if loss<min_loss:\n",
    "                torch.save(model, 'best_model_sig-gpu.pt')\n",
    "                min_loss = loss\n",
    "\n",
    "            if max_acc<(val_correct/len(val_data)):\n",
    "                torch.save(model, 'best_model_acc.pt')\n",
    "                max_acc = val_correct/len(val_data)\n",
    "            \n",
    "            # Save loss for plot\n",
    "            validation_loss.append(loss.cpu().detach().numpy())\n",
    "            \n",
    "            \n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss/train_size)\n",
    "        \n",
    "        \n",
    "        # Save accuracy for plot\n",
    "        training_acc.append(train_correct/len(train_data))\n",
    "        validation_acc.append(val_correct/len(val_data))\n",
    "\n",
    "        # Print loss every 5 epochs\n",
    "        #if i % 5 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "        print(f'Train accuracy = {train_correct/len(train_data)}')\n",
    "        print(f'Validation accuracy = {val_correct/len(val_data)}')\n",
    "\n",
    "        torch.save(model, 'latest_model_sig-gpu.pt')\n",
    "        \n",
    "    if show_plot:\n",
    "        # Plot training and validation loss\n",
    "        epoch = np.arange(len(training_loss))\n",
    "        plt.figure(figsize=(8,4), dpi=100)\n",
    "        plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "        plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot training and validation accuracy\n",
    "        plt.figure(figsize=(8,4), dpi=100)\n",
    "        plt.plot(epoch, training_acc, 'r', label='Training accuracy',)\n",
    "        plt.plot(epoch, validation_acc, 'b', label='Validation accuracy')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch'), plt.ylabel('Accuracy')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 0.9870688343048095, validation loss: 0.6440809965133667\n",
      "Train accuracy = 0.5653846153846154\n",
      "Validation accuracy = 0.6944444444444444\n",
      "Epoch 1, training loss: 0.5372790515422821, validation loss: 0.5908358693122864\n",
      "Train accuracy = 0.7153846153846154\n",
      "Validation accuracy = 0.8055555555555556\n",
      "Epoch 2, training loss: 0.4986774015426636, validation loss: 0.5724557042121887\n",
      "Train accuracy = 0.75\n",
      "Validation accuracy = 0.6805555555555556\n",
      "Epoch 3, training loss: 0.4521308797597885, validation loss: 0.48530319333076477\n",
      "Train accuracy = 0.8192307692307692\n",
      "Validation accuracy = 0.75\n",
      "Epoch 4, training loss: 0.4800739407539368, validation loss: 0.4048489034175873\n",
      "Train accuracy = 0.7807692307692308\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 5, training loss: 0.48147916853427886, validation loss: 0.5996066331863403\n",
      "Train accuracy = 0.7769230769230769\n",
      "Validation accuracy = 0.75\n",
      "Epoch 6, training loss: 0.49827021449804304, validation loss: 0.45844125747680664\n",
      "Train accuracy = 0.8153846153846154\n",
      "Validation accuracy = 0.7777777777777778\n",
      "Epoch 7, training loss: 0.4676281863451004, validation loss: 0.47750696539878845\n",
      "Train accuracy = 0.8115384615384615\n",
      "Validation accuracy = 0.8333333333333334\n",
      "Epoch 8, training loss: 0.526767010986805, validation loss: 0.6284918785095215\n",
      "Train accuracy = 0.7923076923076923\n",
      "Validation accuracy = 0.6388888888888888\n",
      "Epoch 9, training loss: 0.41581945478916166, validation loss: 0.40138500928878784\n",
      "Train accuracy = 0.8076923076923077\n",
      "Validation accuracy = 0.8194444444444444\n",
      "Epoch 10, training loss: 0.3967757377028465, validation loss: 0.4500630795955658\n",
      "Train accuracy = 0.8461538461538461\n",
      "Validation accuracy = 0.7777777777777778\n",
      "Epoch 11, training loss: 0.47975333392620084, validation loss: 0.3047769367694855\n",
      "Train accuracy = 0.7807692307692308\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 12, training loss: 0.34357745945453644, validation loss: 0.462646484375\n",
      "Train accuracy = 0.8423076923076923\n",
      "Validation accuracy = 0.7083333333333334\n",
      "Epoch 13, training loss: 0.43325471848249436, validation loss: 0.37828201055526733\n",
      "Train accuracy = 0.8307692307692308\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 14, training loss: 0.3878245323896408, validation loss: 0.2933669090270996\n",
      "Train accuracy = 0.8153846153846154\n",
      "Validation accuracy = 0.875\n",
      "Epoch 15, training loss: 0.3843602451682091, validation loss: 1.7420276403427124\n",
      "Train accuracy = 0.85\n",
      "Validation accuracy = 0.75\n",
      "Epoch 16, training loss: 0.3775134295225143, validation loss: 0.2788931131362915\n",
      "Train accuracy = 0.8115384615384615\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 17, training loss: 0.3762462395429611, validation loss: 0.28004053235054016\n",
      "Train accuracy = 0.8115384615384615\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 18, training loss: 0.41702053546905515, validation loss: 0.3480105996131897\n",
      "Train accuracy = 0.7923076923076923\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 19, training loss: 0.3922131085395813, validation loss: 0.34518447518348694\n",
      "Train accuracy = 0.8384615384615385\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 20, training loss: 0.3252408143877983, validation loss: 0.28488555550575256\n",
      "Train accuracy = 0.8653846153846154\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 21, training loss: 0.34018314480781553, validation loss: 0.2300490289926529\n",
      "Train accuracy = 0.8692307692307693\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 22, training loss: 0.36369300574064256, validation loss: 0.30141133069992065\n",
      "Train accuracy = 0.8346153846153846\n",
      "Validation accuracy = 0.875\n",
      "Epoch 23, training loss: 0.37980437636375425, validation loss: 0.8557276725769043\n",
      "Train accuracy = 0.823076923076923\n",
      "Validation accuracy = 0.7916666666666666\n",
      "Epoch 24, training loss: 0.39645084023475646, validation loss: 0.34618741273880005\n",
      "Train accuracy = 0.8576923076923076\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 25, training loss: 0.31713065683841707, validation loss: 0.2655075490474701\n",
      "Train accuracy = 0.8653846153846154\n",
      "Validation accuracy = 0.875\n",
      "Epoch 26, training loss: 0.375584407299757, validation loss: 0.3290719985961914\n",
      "Train accuracy = 0.8346153846153846\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 27, training loss: 0.341589173078537, validation loss: 0.32863813638687134\n",
      "Train accuracy = 0.8384615384615385\n",
      "Validation accuracy = 0.8333333333333334\n",
      "Epoch 28, training loss: 0.39468238055706023, validation loss: 0.307826429605484\n",
      "Train accuracy = 0.8576923076923076\n",
      "Validation accuracy = 0.8194444444444444\n",
      "Epoch 29, training loss: 0.24136226922273635, validation loss: 0.2998516261577606\n",
      "Train accuracy = 0.9076923076923077\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 30, training loss: 0.2735866022109985, validation loss: 0.37216630578041077\n",
      "Train accuracy = 0.8846153846153846\n",
      "Validation accuracy = 0.8055555555555556\n",
      "Epoch 31, training loss: 0.3568188512325287, validation loss: 0.26048538088798523\n",
      "Train accuracy = 0.8807692307692307\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 32, training loss: 0.37110029220581053, validation loss: 0.2895045280456543\n",
      "Train accuracy = 0.85\n",
      "Validation accuracy = 0.875\n",
      "Epoch 33, training loss: 0.29121745377779007, validation loss: 0.21177320182323456\n",
      "Train accuracy = 0.8692307692307693\n",
      "Validation accuracy = 0.9166666666666666\n",
      "Epoch 34, training loss: 0.19601855263113976, validation loss: 0.22789593040943146\n",
      "Train accuracy = 0.9\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 35, training loss: 0.31045361801981924, validation loss: 1.0734606981277466\n",
      "Train accuracy = 0.9307692307692308\n",
      "Validation accuracy = 0.5416666666666666\n",
      "Epoch 36, training loss: 0.36188371658325197, validation loss: 0.3036808967590332\n",
      "Train accuracy = 0.8423076923076923\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 37, training loss: 0.32525570631027223, validation loss: 0.3566240668296814\n",
      "Train accuracy = 0.8615384615384616\n",
      "Validation accuracy = 0.8055555555555556\n",
      "Epoch 38, training loss: 0.33652620889246465, validation loss: 0.8905938267707825\n",
      "Train accuracy = 0.8653846153846154\n",
      "Validation accuracy = 0.5833333333333334\n",
      "Epoch 39, training loss: 0.35329467952251437, validation loss: 0.33768415451049805\n",
      "Train accuracy = 0.8461538461538461\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 40, training loss: 0.28976553440093994, validation loss: 0.283444881439209\n",
      "Train accuracy = 0.9038461538461539\n",
      "Validation accuracy = 0.875\n",
      "Epoch 41, training loss: 0.2411305798590183, validation loss: 0.25857824087142944\n",
      "Train accuracy = 0.9076923076923077\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 42, training loss: 0.3298934110999107, validation loss: 0.26147010922431946\n",
      "Train accuracy = 0.8692307692307693\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 43, training loss: 0.21153267070651055, validation loss: 0.38424694538116455\n",
      "Train accuracy = 0.9423076923076923\n",
      "Validation accuracy = 0.8194444444444444\n",
      "Epoch 44, training loss: 0.17213727466762066, validation loss: 0.16141925752162933\n",
      "Train accuracy = 0.9346153846153846\n",
      "Validation accuracy = 0.9166666666666666\n",
      "Epoch 45, training loss: 0.19139810658991338, validation loss: 0.3493248224258423\n",
      "Train accuracy = 0.9307692307692308\n",
      "Validation accuracy = 0.8194444444444444\n",
      "Epoch 46, training loss: 0.18913857266306877, validation loss: 0.16198712587356567\n",
      "Train accuracy = 0.9307692307692308\n",
      "Validation accuracy = 0.9305555555555556\n",
      "Epoch 47, training loss: 0.2631260058283806, validation loss: 0.39924749732017517\n",
      "Train accuracy = 0.9115384615384615\n",
      "Validation accuracy = 0.7083333333333334\n",
      "Epoch 48, training loss: 0.16496176775544882, validation loss: 0.25163376331329346\n",
      "Train accuracy = 0.9423076923076923\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 49, training loss: 0.34001545399427413, validation loss: 0.30138418078422546\n",
      "Train accuracy = 0.8730769230769231\n",
      "Validation accuracy = 0.875\n",
      "Epoch 50, training loss: 0.28366635993123057, validation loss: 0.17710614204406738\n",
      "Train accuracy = 0.8846153846153846\n",
      "Validation accuracy = 0.9166666666666666\n",
      "Epoch 51, training loss: 0.23839280426502227, validation loss: 0.30910834670066833\n",
      "Train accuracy = 0.926923076923077\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 52, training loss: 0.20085021801292896, validation loss: 0.3695192337036133\n",
      "Train accuracy = 0.9038461538461539\n",
      "Validation accuracy = 0.7916666666666666\n",
      "Epoch 53, training loss: 0.1840145668387413, validation loss: 0.6034354567527771\n",
      "Train accuracy = 0.9346153846153846\n",
      "Validation accuracy = 0.6527777777777778\n",
      "Epoch 54, training loss: 0.21674796123057605, validation loss: 0.6439521908760071\n",
      "Train accuracy = 0.9\n",
      "Validation accuracy = 0.5972222222222222\n",
      "Epoch 55, training loss: 0.20756358794867993, validation loss: 0.29770851135253906\n",
      "Train accuracy = 0.9076923076923077\n",
      "Validation accuracy = 0.8333333333333334\n",
      "Epoch 56, training loss: 0.20310561180114747, validation loss: 0.39305970072746277\n",
      "Train accuracy = 0.9192307692307692\n",
      "Validation accuracy = 0.7638888888888888\n",
      "Epoch 57, training loss: 0.16663594126701356, validation loss: 0.16912078857421875\n",
      "Train accuracy = 0.9461538461538461\n",
      "Validation accuracy = 0.9444444444444444\n",
      "Epoch 58, training loss: 0.14335248503834008, validation loss: 0.18372608721256256\n",
      "Train accuracy = 0.9384615384615385\n",
      "Validation accuracy = 0.9305555555555556\n",
      "Epoch 59, training loss: 0.10013496205210685, validation loss: 0.1998671442270279\n",
      "Train accuracy = 0.9615384615384616\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 60, training loss: 0.2086949609592557, validation loss: 0.6088675260543823\n",
      "Train accuracy = 0.8961538461538462\n",
      "Validation accuracy = 0.875\n",
      "Epoch 61, training loss: 0.21686349373310804, validation loss: 0.26907533407211304\n",
      "Train accuracy = 0.9230769230769231\n",
      "Validation accuracy = 0.8333333333333334\n",
      "Epoch 62, training loss: 0.2733279675245285, validation loss: 0.2932339012622833\n",
      "Train accuracy = 0.8807692307692307\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 63, training loss: 0.18022763799875974, validation loss: 0.38188451528549194\n",
      "Train accuracy = 0.9307692307692308\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 64, training loss: 0.17376526303589343, validation loss: 0.4264463484287262\n",
      "Train accuracy = 0.9307692307692308\n",
      "Validation accuracy = 0.8055555555555556\n",
      "Epoch 65, training loss: 0.15725624233484267, validation loss: 0.2366783320903778\n",
      "Train accuracy = 0.9461538461538461\n",
      "Validation accuracy = 0.875\n",
      "Epoch 66, training loss: 0.1991563879698515, validation loss: 0.37115147709846497\n",
      "Train accuracy = 0.926923076923077\n",
      "Validation accuracy = 0.7916666666666666\n",
      "Epoch 67, training loss: 0.2031680354475975, validation loss: 0.36917638778686523\n",
      "Train accuracy = 0.926923076923077\n",
      "Validation accuracy = 0.7916666666666666\n",
      "Epoch 68, training loss: 0.23553133636713028, validation loss: 0.19671876728534698\n",
      "Train accuracy = 0.9038461538461539\n",
      "Validation accuracy = 0.9305555555555556\n",
      "Epoch 69, training loss: 0.1324447246454656, validation loss: 0.23794010281562805\n",
      "Train accuracy = 0.9692307692307692\n",
      "Validation accuracy = 0.9305555555555556\n",
      "Epoch 70, training loss: 0.21328232666477562, validation loss: 0.2274550348520279\n",
      "Train accuracy = 0.926923076923077\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 71, training loss: 0.1663147634267807, validation loss: 0.47564348578453064\n",
      "Train accuracy = 0.9461538461538461\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 72, training loss: 0.13669832359999418, validation loss: 0.20379969477653503\n",
      "Train accuracy = 0.9423076923076923\n",
      "Validation accuracy = 0.9166666666666666\n",
      "Epoch 73, training loss: 0.09039065869525074, validation loss: 0.3219200670719147\n",
      "Train accuracy = 0.9730769230769231\n",
      "Validation accuracy = 0.875\n",
      "Epoch 74, training loss: 0.21625187246128916, validation loss: 0.3476129174232483\n",
      "Train accuracy = 0.926923076923077\n",
      "Validation accuracy = 0.8333333333333334\n",
      "Epoch 75, training loss: 0.27591091856360433, validation loss: 0.38645777106285095\n",
      "Train accuracy = 0.8961538461538462\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 76, training loss: 0.2418313080072403, validation loss: 0.33692726492881775\n",
      "Train accuracy = 0.9153846153846154\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 77, training loss: 0.15758286003023386, validation loss: 0.23343701660633087\n",
      "Train accuracy = 0.95\n",
      "Validation accuracy = 0.9027777777777778\n",
      "Epoch 78, training loss: 0.08336687202565372, validation loss: 0.1947050392627716\n",
      "Train accuracy = 0.9730769230769231\n",
      "Validation accuracy = 0.9166666666666666\n",
      "Epoch 79, training loss: 0.10291817692108453, validation loss: 0.22404424846172333\n",
      "Train accuracy = 0.9730769230769231\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 80, training loss: 0.16642559587955474, validation loss: 0.22791868448257446\n",
      "Train accuracy = 0.9538461538461539\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 81, training loss: 0.16523031286895276, validation loss: 0.4163578450679779\n",
      "Train accuracy = 0.9384615384615385\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 82, training loss: 0.1976804295927286, validation loss: 0.3481632173061371\n",
      "Train accuracy = 0.9307692307692308\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 83, training loss: 0.0812664757668972, validation loss: 0.489647775888443\n",
      "Train accuracy = 0.9615384615384616\n",
      "Validation accuracy = 0.7916666666666666\n",
      "Epoch 84, training loss: 0.16326123412698507, validation loss: 0.4373144507408142\n",
      "Train accuracy = 0.9615384615384616\n",
      "Validation accuracy = 0.8194444444444444\n",
      "Epoch 85, training loss: 0.13746267320588232, validation loss: 0.3078388273715973\n",
      "Train accuracy = 0.9615384615384616\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 86, training loss: 0.10377989886328579, validation loss: 0.3752419054508209\n",
      "Train accuracy = 0.9692307692307692\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 87, training loss: 0.10907353714108467, validation loss: 0.30309033393859863\n",
      "Train accuracy = 0.9576923076923077\n",
      "Validation accuracy = 0.8472222222222222\n",
      "Epoch 88, training loss: 0.12443906400352717, validation loss: 0.3870009481906891\n",
      "Train accuracy = 0.95\n",
      "Validation accuracy = 0.8333333333333334\n",
      "Epoch 89, training loss: 0.05917998038232326, validation loss: 0.24991199374198914\n",
      "Train accuracy = 0.9730769230769231\n",
      "Validation accuracy = 0.875\n",
      "Epoch 90, training loss: 0.059765296732075514, validation loss: 0.37412282824516296\n",
      "Train accuracy = 0.9807692307692307\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 91, training loss: 0.07667269255965949, validation loss: 0.5007641911506653\n",
      "Train accuracy = 0.9692307692307692\n",
      "Validation accuracy = 0.8611111111111112\n",
      "Epoch 92, training loss: 0.08540121676400304, validation loss: 0.2407902032136917\n",
      "Train accuracy = 0.9653846153846154\n",
      "Validation accuracy = 0.9166666666666666\n",
      "Epoch 93, training loss: 0.05841607891954482, validation loss: 0.28300943970680237\n",
      "Train accuracy = 0.9807692307692307\n",
      "Validation accuracy = 0.875\n",
      "Epoch 94, training loss: 0.12179649153258651, validation loss: 0.2947961091995239\n",
      "Train accuracy = 0.9576923076923077\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 95, training loss: 0.07232878763228655, validation loss: 0.2509523332118988\n",
      "Train accuracy = 0.9807692307692307\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 96, training loss: 0.042150439568795266, validation loss: 0.23593123257160187\n",
      "Train accuracy = 0.9846153846153847\n",
      "Validation accuracy = 0.8888888888888888\n",
      "Epoch 97, training loss: 0.052036084339488295, validation loss: 0.3253713846206665\n",
      "Train accuracy = 0.9807692307692307\n",
      "Validation accuracy = 0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# Hyperparams\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "\n",
    "# define network\n",
    "network = models.resnet18(num_classes = 1)\n",
    "network.conv1 = nn.Conv2d(12, 64, kernel_size=3, stride=1, padding=3) # 12 bands = 12 input channels\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "network.to(device)\n",
    "\n",
    "# define loss\n",
    "loss_function = torch.nn.BCEWithLogitsLoss().to(device)\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_model(network, loss_function, optimizer, train_loader, val_loader, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "correct = 0\n",
    "predictions = []\n",
    "cloudy_pred = 0\n",
    "clear_pred = 0\n",
    "\n",
    "model = torch.load('best_model_sig-gpu.pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_nr, (data, labels) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # predict\n",
    "            pred = model(data)\n",
    "            pred = torch.squeeze(pred)\n",
    "        \n",
    "            # calculate accuracy\n",
    "            preds = torch.sigmoid(pred) >= 0.5\n",
    "            \n",
    "            for pred in preds:\n",
    "                predictions.append(pred.cpu())\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    if pred == test_label[idx]:\n",
    "        correct += 1\n",
    "        if pred == 1:\n",
    "            cloudy_pred += 1\n",
    "        else:\n",
    "            clear_pred += 1\n",
    "\n",
    "print(\"Final accuracy: %.2f%%\" % (100*correct/len(test_label)))\n",
    "print(f'Correct {correct} times out of {len(img_paths_test)}')\n",
    "\n",
    "\n",
    "total_cloudy = np.sum(test_label)\n",
    "total_clear = len(test_label)-total_cloudy\n",
    "\n",
    "print(f'Correct {clear_pred} times out of {total_clear}: {100*clear_pred/total_clear}')\n",
    "print(f'Correct {cloudy_pred} times out of {total_cloudy}: {100*cloudy_pred/total_cloudy}')\n",
    "\n",
    "cm = confusion_matrix(test_label, predictions)\n",
    "ConfusionMatrixDisplay(confusion_matrix = cm,  display_labels=['Clear', 'Cloudy']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_clear = clear_pred/total_clear\n",
    "recall_cloudy = cloudy_pred/total_cloudy\n",
    "precision_clear = clear_pred/(clear_pred+(total_cloudy-cloudy_pred))\n",
    "precision_cloudy = cloudy_pred/(cloudy_pred+(total_clear-clear_pred))\n",
    "f1_clear = (2*precision_clear*recall_clear)/(precision_clear+recall_clear)\n",
    "f1_cloudy = (2*precision_cloudy*recall_cloudy)/(precision_cloudy+recall_cloudy)\n",
    "\n",
    "f1_avg = (f1_clear+f1_cloudy)/2\n",
    "rec_avg = (recall_clear+recall_cloudy)/2\n",
    "prec_avg = (precision_clear+precision_cloudy)/2\n",
    "\n",
    "print(f'f1 avg {f1_avg:.4f}')\n",
    "\n",
    "print(f'f1 clear {f1_clear:.4f}')\n",
    "\n",
    "print(f'f1 cloudy {f1_cloudy:.4f}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'recall avg {rec_avg:.4f}')\n",
    "\n",
    "print(f'recall clear {recall_clear:.4f}')\n",
    "\n",
    "print(f'recall cloudy {recall_cloudy:.4f}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'precision avg {prec_avg:.4f}')\n",
    "\n",
    "print(f'precision clear {precision_clear:.4f}')\n",
    "\n",
    "print(f'precision cloudy {precision_cloudy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Article       | ResNet18       |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| **f1-avg** | 0.90 | **0.95**|\n",
    "| **f1-clear** | 0.94 | **0.97**|\n",
    "| **f1-cloudy** | 0.86 |**0.93**|\n",
    "| **rec-avg** | 0.94 |**0.94** |\n",
    "| **rec-clear** | 0.91 |**0.99** |\n",
    "| **rec-cloudy** | 0.88 | **0.89**|\n",
    "| **prec-avg** | 0.91 |**0.96** |\n",
    "| **prec-clear** | 0.95 |**0.96** |\n",
    "| **prec-cloudy** | 0.85 |**0.96**|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | MLP-5       | MLP-5-ens-10       | ResNet18-cls    |     | MLP       |CNN       |ResNet18       |\n",
    "| ----------- | ----------- | ----------- |----------- |----------- |----------- |----------- |----------- |\n",
    "| **f1-avg** | 0.73 | 0.88|0.90| | 0.89 | 0.83|**0.95**|\n",
    "| **f1-clear** | 0.81 | 0.94|0.94|| 0.94 | 0.91|**0.97**|\n",
    "| **f1-cloudy** | 0.65 |0.82|0.86|| 0.84 | 0.75|**0.93**|\n",
    "| **rec-avg** | 0.73 |0.86 |0.91||0.89 | 0.82|**0.94**|\n",
    "| **rec-clear** | 0.77 |0.97 |0.94||0.93 | 0.93|**0.99**|\n",
    "| **rec-cloudy** | 0.68 | 0.75|0.88||0.85 | 0.71|**0.89**|\n",
    "| **prec-avg** | 0.74 |0.91 |0.90||0.88 | 0.85|**0.96**|\n",
    "| **prec-clear** | 0.85 |0.91 |0.95||0.94 | 0.89|**0.96**|\n",
    "| **prec-cloudy** | 0.63 |0.91|0.85||0.82 | 0.80|**0.96**|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the project work\n",
    "\n",
    "Steps:\n",
    "\n",
    "0. Import packages\n",
    "1. Loading the data \n",
    "2. Ploting an example picture \n",
    "3. Creating the network\n",
    "4. training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the data\n",
    "\n",
    "-Looks like the data is already splited when we get it so we don't need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_DATA = './skogsstyrelsen/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data + corresponding json info (incl ground truth)\n",
    "img_paths_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_train.npy')))\n",
    "img_paths_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_val.npy')))\n",
    "img_paths_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_names_test.npy')))\n",
    "json_content_train = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_train.npy'), allow_pickle=True))\n",
    "json_content_val = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_val.npy'), allow_pickle=True))\n",
    "json_content_test = list(np.load(os.path.join(BASE_PATH_DATA, 'skogs_json_test.npy'), allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training split of the MNIST dataset, the ToTensor makes sure we get the data as tensors, not images\n",
    "mnist_train = torchvision.datasets.MNIST('./', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST('./', train=False, download=True, transform=torchvision.transforms.ToTensor())\n",
    "# Create a DataLoader from the dataset that we'll use to get batches of data during training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=1000, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Ploting an example picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that takes an MNIST tensor and shows the image\n",
    "def plot_digit(data):\n",
    "    # Transfrom the images into an appropriate shape for displaying\n",
    "    data = data.view(28,28)\n",
    "    plt.imshow(data, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Get the first batch of images and labels from the DataLoader\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Plot the first image of the batch\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Creating the network\n",
    "\n",
    "-\"CrossEntropyLoss expects class indices and does not take one-hot encoded tensors as target labels.\" pytorch forums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------Network--------------------------------------\n",
    "\n",
    "epochs=300\n",
    "network=nn.Sequential(\n",
    "    nn.Linear(784, 80), # First layer of the network takes the entire image and reduces it to 100 dimensions\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(80, 10) # The second layer takes those 100 dimensions and reduces them into estimeated values for each digit\n",
    ")\n",
    "\n",
    "loss_function=torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide the number of epochs to train for (one epoch is one optimization iteration on the entire dataset)\n",
    "epochs = 10\n",
    "losses=[]\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # For each batch of data (since the dataset is too large to run all data through the network at once)\n",
    "    for (images, labels) in train_loader:\n",
    "\n",
    "        # Extract the labels and turn them into one-hot representation (note: not all loss functions needs this)\n",
    "\n",
    "        # Reshape the images to a single vector (28*28 = 784)\n",
    "        images = images.view(-1,784)\n",
    "\n",
    "        # Predict for each vector what digit they represent\n",
    "        prediction = network(data)\n",
    "\n",
    "        # Calculate the loss of the prediction by comparing to the expected output\n",
    "        loss = loss_function(prediction, labels)\n",
    "\n",
    "        # Backpropogate the loss through the network to find the gradients of all parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters along their gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clear stored gradient values\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "        losses.append(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
